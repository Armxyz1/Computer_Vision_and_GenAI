{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFNkvjgn1lPI"
      },
      "source": [
        "## Fine-tune Stable DiffusionXL with DreamBooth and LoRA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdM59WIoS01O"
      },
      "source": [
        "#### Step 1: Install libraries and dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55U57alsRIek"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes transformers accelerate peft -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJXRjGZa1vFZ"
      },
      "source": [
        "Get the diffusers repository from HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9atNWUXFZlVe"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/diffusers.git -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2TRPaxGx3mE"
      },
      "source": [
        "Download diffusers SDXL DreamBooth training script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQpqmq7rx3mE"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/diffusers/main/examples/dreambooth/train_dreambooth_lora_sdxl.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRBcvYAyS8bU"
      },
      "source": [
        "#### Step 2: Training data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a folder named `SDXL_train` and upload all your training images in it. Make sure all the images are either `.png` or `.jpg`"
      ],
      "metadata": {
        "id": "SURDyDkGy2UV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqqhZ9R9x3mG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "local_dir = \"./SDXL_train/\"\n",
        "os.makedirs(local_dir)\n",
        "os.chdir(local_dir)\n",
        "\n",
        "# Browse the images from your computer and upload it into the SDXL_train directory.\n",
        "uploaded_images = files.upload()\n",
        "os.chdir(\"/content\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piJkbOe9OZbX"
      },
      "source": [
        "Preview the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0R-hByAPkIg"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def image_grid(imgs, rows, cols, resize=256):\n",
        "\n",
        "    if resize is not None:\n",
        "        imgs = [img.resize((resize, resize)) for img in imgs]\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwhwepFrPwqo"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "# change path to display images from your local dir\n",
        "img_paths = \"./SDXL_train/*.jpg\"\n",
        "imgs = [Image.open(path) for path in glob.glob(img_paths)]\n",
        "\n",
        "num_imgs_to_preview = 5\n",
        "image_grid(imgs[:num_imgs_to_preview], 1, num_imgs_to_preview)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqrxnDMUTADH"
      },
      "source": [
        "**Generate custom captions with BLIP**\n",
        "\n",
        "Load BLIP to auto caption your images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZK1QuLqx3mH"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# load the processor and the captioning model\n",
        "blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\",torch_dtype=torch.float16).to(device)\n",
        "\n",
        "# captioning utility\n",
        "def caption_images(input_image):\n",
        "    inputs = blip_processor(images=input_image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "    pixel_values = inputs.pixel_values\n",
        "\n",
        "    generated_ids = blip_model.generate(pixel_values=pixel_values, max_length=50)\n",
        "    generated_caption = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return generated_caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfkwE439x3mH"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "# create a list of (Pil.Image, path) pairs\n",
        "local_dir = \"./SDXL_train/\"\n",
        "imgs_and_paths = [(path,Image.open(path)) for path in glob.glob(f\"{local_dir}*.jpg\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V9cYeVDx3mH"
      },
      "source": [
        "#### Add a token identifier\n",
        "\n",
        "Let's incorporate a token identifier (e.g., TOK) into each caption by introducing a caption prefix.\n",
        "\n",
        "Feel free to customize the prefix based on the specific concept you are training on.\n",
        "\n",
        "For example: If you're fine-tuning on -\n",
        "*   Human face - \"Photo of a TOK person\"\n",
        "*   Car - \"A photo of a TOK car\"\n",
        "*   Styles - \"In the style of watercolour\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJcZyeSVx3mH"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "caption_prefix = \"a photo of TOK home, \"\n",
        "with open(f'{local_dir}metadata.jsonl', 'w') as outfile:\n",
        "  for img in imgs_and_paths:\n",
        "      caption = caption_prefix + caption_images(img[1]).split(\"\\n\")[0]\n",
        "      entry = {\"file_name\":img[0].split(\"/\")[-1], \"prompt\": caption}\n",
        "      json.dump(entry, outfile)\n",
        "      outfile.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_5Skm_cx3mI"
      },
      "source": [
        "Free up some memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4wfEW_cx3mI"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "# Delete the BLIP pipelines and free up some memory.\n",
        "del blip_processor, blip_model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLptJV1vx3mI"
      },
      "source": [
        "#### Step 3: Start training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7OzF0i716Kf"
      },
      "source": [
        "Initialize `accelerate`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WW7eFPl4eYXy"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!accelerate config default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az-IQG-Ax3mJ"
      },
      "source": [
        "#### Setting the hyperparameters\n",
        "To ensure seamless integration of DreamBooth with LoRA on a resource-intensive pipeline like Stable Diffusion XL, we are implementing the following techniques:\n",
        "\n",
        "* Gradient checkpointing (`--gradient_accumulation_steps`)\n",
        "* 8-bit Adam (`--use_8bit_adam`)\n",
        "* Mixed-precision training (`--mixed-precision=\"fp16\"`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIMwkmL7N82R"
      },
      "source": [
        "For custom captions, the installation of the `datasets` library is required. However, if you intend to train exclusively with `--instance_prompt`, you have the option to skip this step. In such instances, please specify `--instance_data_dir` instead of `--dataset_name`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTZuM7yNNZsT"
      },
      "outputs": [],
      "source": [
        "!pip install datasets -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Specify your LoRA model repository name using `--output_dir`.\n",
        " - Use `--caption_column` to indicate the name of the caption column in your dataset.\n",
        "\n",
        " In this example, \"prompt\" was used to save captions in the metadata file; feel free to modify this based on your requirements."
      ],
      "metadata": {
        "collapsed": false,
        "id": "fbri7qQ0mdyg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOiZSXHfx3mJ"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env bash\n",
        "!accelerate launch train_dreambooth_lora_sdxl.py \\\n",
        "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
        "  --pretrained_vae_model_name_or_path=\"madebyollin/sdxl-vae-fp16-fix\" \\\n",
        "  --dataset_name=\"SDXL_train\" \\\n",
        "  --output_dir=\"SDXL_LoRA_model\" \\\n",
        "  --caption_column=\"prompt\"\\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --instance_prompt=\"a photo of TOK home\" \\\n",
        "  --resolution=1024 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=3 \\\n",
        "  --gradient_checkpointing \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --snr_gamma=5.0 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --max_train_steps=500 \\\n",
        "  --checkpointing_steps=717 \\\n",
        "  --seed=\"0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH7-YJwMcyra"
      },
      "source": [
        "#### Step 4: Inference model\n",
        "Load the fine-tuned LoRA weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTz6Zmfc0i-0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import DiffusionPipeline, AutoencoderKL\n",
        "\n",
        "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    vae=vae,\n",
        "    torch_dtype=torch.float16,\n",
        "    variant=\"fp16\",\n",
        "    use_safetensors=True\n",
        ")\n",
        "pipe.load_lora_weights('/content/SDXL_LoRA_model/pytorch_lora_weights.safetensors')\n",
        "_ = pipe.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text to Image Generation"
      ],
      "metadata": {
        "id": "FWV-r8_VRP87"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J1NsUP51E2w"
      },
      "outputs": [],
      "source": [
        "prompt = \"A photo of TOK home, an Indian living room basking in Republic Day morning sun, adorned with saffron, white, and green decor, and vibrant festive accents.\"\n",
        "\n",
        "image = pipe(prompt=prompt, num_inference_steps=25).images[0]\n",
        "\n",
        "# Save the SDXL image output.\n",
        "image.save('/content/sdxl_output.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Image to Image Generation"
      ],
      "metadata": {
        "id": "c4rrBud9RgjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the fine-tuned LoRA weights."
      ],
      "metadata": {
        "id": "7FUOsDaZY8-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import AutoPipelineForImage2Image\n",
        "from diffusers.utils import make_image_grid, load_image\n",
        "\n",
        "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
        ")\n",
        "pipeline.load_lora_weights('/content/SDXL_LoRA_model/pytorch_lora_weights.safetensors')\n",
        "_ = pipeline.to(\"cuda\")\n",
        "\n",
        "pipeline.enable_model_cpu_offload()"
      ],
      "metadata": {
        "id": "4GksN5r4RnOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Image-to-image generation might possibly require a runtime restart."
      ],
      "metadata": {
        "id": "IFaH3E6x9QF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the text-to-image generated output as an input to image-to-image.\n",
        "image2image = load_image('/content/sdxl_output.png')\n",
        "resized_image = image2image.resize((1024, 576))\n",
        "\n",
        "prompt = \"A photo of TOK home, an Indian living room basking in Republic Day morning sun, adorned with saffron, white, and green decor, and vibrant festive accents.\"\n",
        "\n",
        "# pass prompt and image to pipeline\n",
        "image_output = pipeline(prompt, image=resized_image, strength=0.5).images[0]\n",
        "make_image_grid([image, image_output], rows=1, cols=2)"
      ],
      "metadata": {
        "id": "GWBevyGnZF9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stable Video Diffusion (Image to Video)\n",
        "\n",
        "Now, we pass the output of SDXL as an input to SVD to generate a video of the created image."
      ],
      "metadata": {
        "id": "SKMajn13RFK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U diffusers transformers accelerate"
      ],
      "metadata": {
        "id": "EYjv7AalRE2R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6cf9113-604c-454f-9a83-bf247bfcec7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from diffusers import StableVideoDiffusionPipeline\n",
        "from diffusers.utils import load_image, export_to_video\n",
        "\n",
        "pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-video-diffusion-img2vid-xt\", torch_dtype=torch.float16, variant=\"fp16\"\n",
        ")\n",
        "\n",
        "# pipe.enable_model_cpu_offload()\n",
        "# pipe.to(\"cuda\")\n",
        "\n",
        "pipe.enable_model_cpu_offload()\n",
        "pipe.unet.enable_forward_chunking()\n",
        "\n",
        "\n",
        "# Load the conditioning image.\n",
        "image = load_image(\"/content/sdxl_output.png\")\n",
        "resized_image = image.resize((1024, 576))\n",
        "\n",
        "generator = torch.manual_seed(42)\n",
        "# frames = pipe(image, decode_chunk_size=8, generator=generator).frames[0]\n",
        "frames = pipe(resized_image, decode_chunk_size=2, generator=generator, num_frames=25).frames[0]\n",
        "\n",
        "export_to_video(frames, \"generated.mp4\", fps=7)"
      ],
      "metadata": {
        "id": "6tQLrTtIRJa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SVD with micro-conditioning"
      ],
      "metadata": {
        "id": "CqCZtfUORU2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
        "  \"stabilityai/stable-video-diffusion-img2vid-xt\", torch_dtype=torch.float16, variant=\"fp16\"\n",
        ")\n",
        "pipe.enable_model_cpu_offload()\n",
        "\n",
        "# Load the conditioning image.\n",
        "image1 = load_image(\"/content/sdxl_output.png\")\n",
        "image1 = image1.resize((1024, 576))\n",
        "\n",
        "generator = torch.manual_seed(42)\n",
        "frames = pipe(image1, decode_chunk_size=8, generator=generator, motion_bucket_id=180, noise_aug_strength=0.1).frames[0]\n",
        "export_to_video(frames, \"generated_micro_conditioning.mp4\", fps=7)"
      ],
      "metadata": {
        "id": "H12G1AKZROKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 5: Model Deployment on HF *Spaces*\n",
        "\n",
        "The fine-tuned SDXL model with LoRA weights can be deployed on HuggingFace Hub and hosted as an application on HF Spaces."
      ],
      "metadata": {
        "id": "GYL-niu6Ov2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "po89ws2ik1cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Login to your HuggingFace account, go to your profile settings --> Access Tokens --> copy the access token and paste in the box below."
      ],
      "metadata": {
        "id": "w7pl3a8C6IY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import whoami\n",
        "from pathlib import Path\n",
        "\n",
        "# Output directory.\n",
        "output_dir = \"SDXL_LoRA_model\"\n",
        "username = whoami(token=Path(\"/root/.cache/huggingface/\"))[\"name\"]\n",
        "repo_id = f\"{username}/{output_dir}\""
      ],
      "metadata": {
        "id": "5Pe5LavS29g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from train_dreambooth_lora_sdxl import save_model_card\n",
        "from huggingface_hub import upload_folder, create_repo\n",
        "\n",
        "repo_id = create_repo(repo_id, exist_ok=True).repo_id\n",
        "\n",
        "# change the params below according to your training arguments\n",
        "save_model_card(\n",
        "    repo_id = repo_id,\n",
        "    images=[],\n",
        "    base_model=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    train_text_encoder=False,\n",
        "    instance_prompt=\"a photo of TOK home\",\n",
        "    validation_prompt=None,\n",
        "    repo_folder=output_dir,\n",
        "    vae_path=\"madebyollin/sdxl-vae-fp16-fix\",\n",
        ")\n",
        "\n",
        "upload_folder(\n",
        "    repo_id=repo_id,\n",
        "    folder_path=output_dir,\n",
        "    commit_message=\"End of training\",\n",
        "    ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
        ")"
      ],
      "metadata": {
        "id": "L2Pzqcgh2yBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 6: On-device deployment using MediaPipe Image Generator Task\n",
        "(under development)"
      ],
      "metadata": {
        "id": "ohO0NzPy9HzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get the foundation model"
      ],
      "metadata": {
        "id": "DyrTK6XjPYs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import HuggingFace Hub.\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Download the v1-5-pruned-emaonly foundation diffusion model checkpoint.\n",
        "hf_hub_download(repo_id=\"runwayml/stable-diffusion-v1-5\", filename=\"v1-5-pruned-emaonly.ckpt\", local_dir=\"/content/\")"
      ],
      "metadata": {
        "id": "a8lHRV9UOyqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries.\n",
        "!pip install torch typing_extensions numpy Pillow requests pytorch_lightning absl-py"
      ],
      "metadata": {
        "id": "WjY6D0-_PnmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convert the diffusion model checkpoint to MediaPipe Image Generator format."
      ],
      "metadata": {
        "id": "YrHPSakCfJKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the Python script to convert the diffusion checkpoints to a format suitable for on-device deployment.\n",
        "!wget https://raw.githubusercontent.com/NSTiwari/Stable-DiffusionXL-using-DreamBooth-and-LoRA-on-Android/main/convert_diffusion_checkpoints.py\n",
        "\n",
        "# Create a directory for model.\n",
        "os.makedirs('./model')\n",
        "\n",
        "# Convert the foundation diffusion model checkpoints to the Image Generator format.\n",
        "!python3 convert_diffusion_checkpoints.py --ckpt_path /content/v1-5-pruned-emaonly.ckpt --output_path /content/model/"
      ],
      "metadata": {
        "id": "neZDgjb4Q2Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the converted foundation model.\n",
        "!zip -r /content/model.zip /content/model/"
      ],
      "metadata": {
        "id": "UJCwNOjoet4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('/content/model.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "vpyEtV8f0D8C",
        "outputId": "f3640cd7-006b-4b2d-d791-0289e0a2e94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b299b931-bb4a-454a-b875-d3289a1521bd\", \"model.zip\", 1906248841)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}